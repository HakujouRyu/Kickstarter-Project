{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hakuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hakuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "# import eli5 # permutation imprtance\n",
    "# from eli5.sklearn import PermutationImportance\n",
    "# from xgboost import XGBClassifier\n",
    "import nltk # Natural language\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from bs4 import BeautifulSoup #to strip url\n",
    "import string # for a list of puntuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_some(year):\n",
    "    df = pd.DataFrame(\n",
    "            columns=['backers_count', 'blurb', 'category', 'converted_pledged_amount',\n",
    "       'country', 'created_at', 'creator', 'currency', 'currency_symbol',\n",
    "       'currency_trailing_code', 'current_currency', 'deadline',\n",
    "       'disable_communication', 'fx_rate', 'goal', 'id', 'is_starrable',\n",
    "       'launched_at', 'name', 'photo', 'pledged', 'profile', 'slug',\n",
    "       'source_url', 'spotlight', 'staff_pick', 'state', 'state_changed_at',\n",
    "       'static_usd_rate', 'urls', 'usd_pledged', 'usd_type', 'location',\n",
    "       'friends', 'is_backing', 'is_starred', 'permissions']\n",
    "    )\n",
    "    folders = os.listdir(f'Data\\\\{year}') #Get the monthly folders inside the year\n",
    "    for folder in folders[:1]:\n",
    "        files = os.listdir(f'Data\\\\{year}\\\\{folder}')  #Get the filenames inside monthly folders\n",
    "        monthly = pd.concat(\n",
    "            [pd.read_csv(\n",
    "                f'Data\\\\{year}\\\\{folder}\\\\{file}') for file in files[:2]] #Not getting a whole year for now\n",
    "        ) #Reads in all the csv files in a given month\n",
    "        df = df.append(monthly)\n",
    "        df = df.reset_index().drop(columns='index')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_year(year):\n",
    "    df = pd.DataFrame(\n",
    "            columns=['backers_count', 'blurb', 'category', 'converted_pledged_amount',\n",
    "       'country', 'created_at', 'creator', 'currency', 'currency_symbol',\n",
    "       'currency_trailing_code', 'current_currency', 'deadline',\n",
    "       'disable_communication', 'fx_rate', 'goal', 'id', 'is_starrable',\n",
    "       'launched_at', 'name', 'photo', 'pledged', 'profile', 'slug',\n",
    "       'source_url', 'spotlight', 'staff_pick', 'state', 'state_changed_at',\n",
    "       'static_usd_rate', 'urls', 'usd_pledged', 'usd_type', 'location',\n",
    "       'friends', 'is_backing', 'is_starred', 'permissions']\n",
    "    )\n",
    "    folders = os.listdir(f'Data\\\\{year}') #Get the monthly folders inside the year\n",
    "    for folder in folders:\n",
    "        files = os.listdir(f'Data\\\\{year}\\\\{folder}')  #Get the filenames inside monthly folders\n",
    "        monthly = pd.concat(\n",
    "            [pd.read_csv(\n",
    "                f'Data\\\\{year}\\\\{folder}\\\\{file}') for file in files]\n",
    "        ) #Reads in all the csv files in a given month\n",
    "        df = df.append(monthly)\n",
    "        df = df.reset_index().drop(columns='index')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_few(year):\n",
    "    df = pd.DataFrame(\n",
    "            columns=['backers_count', 'blurb', 'category', 'converted_pledged_amount',\n",
    "       'country', 'created_at', 'creator', 'currency', 'currency_symbol',\n",
    "       'currency_trailing_code', 'current_currency', 'deadline',\n",
    "       'disable_communication', 'fx_rate', 'goal', 'id', 'is_starrable',\n",
    "       'launched_at', 'name', 'photo', 'pledged', 'profile', 'slug',\n",
    "       'source_url', 'spotlight', 'staff_pick', 'state', 'state_changed_at',\n",
    "       'static_usd_rate', 'urls', 'usd_pledged', 'usd_type', 'location',\n",
    "       'friends', 'is_backing', 'is_starred', 'permissions']\n",
    "    )\n",
    "    folders = os.listdir(f'Data\\\\{year}') #Get the monthly folders inside the year\n",
    "    for folder in folders[:1]: #Grab a folder from that year\n",
    "        files = os.listdir(f'Data\\\\{year}\\\\{folder}')  #Get the filenames inside monthly folders\n",
    "        monthly = pd.concat(\n",
    "            [pd.read_csv(\n",
    "                f'Data\\\\{year}\\\\{folder}\\\\{file}') for file in files[:1]] #Grab a file\n",
    "        ) #Reads in all the csv files in a given month\n",
    "        df = df.append(monthly)\n",
    "        df = df.reset_index().drop(columns='index')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_convert(df):\n",
    "    #Time is in seconds (epoch)\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], unit='s')\n",
    "    df['deadline'] = pd.to_datetime(df['deadline'], unit='s')\n",
    "    df['launched_at'] = pd.to_datetime(df['launched_at'], unit='s')\n",
    "    # df['state_changed_at'] = pd.to_datetime(df['state_changed_at'], unit='s') Leakage for current project goals\n",
    "\n",
    "    #Break time up into columns Month day etc\n",
    "    df['month_started'] = df['created_at'].dt.month\n",
    "    df['day_started'] = df['created_at'].dt.weekday\n",
    "    df['year_started'] = df['created_at'].dt.year\n",
    "    df['month_launched'] = df['launched_at'].dt.month\n",
    "    df['day_launched'] = df['launched_at'].dt.weekday\n",
    "    df['year_launched'] = df['launched_at'].dt.year\n",
    "    df['deadline_month'] = df['deadline'].dt.month\n",
    "    df['deadline day'] = df['deadline'].dt.weekday\n",
    "    df['deadline_year'] = df['deadline'].dt.year\n",
    "\n",
    "    #Feature engineering\n",
    "    df['days_to_launch'] = (df['launched_at'] - df['created_at']).dt.days\n",
    "    df['campaign_length'] = (df['deadline'] - df['launched_at']).dt.days #campaign length in days\n",
    "    \n",
    "    return df\n",
    "\n",
    "def time_to_string(df):\n",
    "    #Convert back into strings so that we can pass to model\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], format='%m%d%Y').astype(str)\n",
    "    df['deadline'] = pd.to_datetime(df['deadline'], format='%m%d%Y').astype(str)\n",
    "    df['launched_at'] = pd.to_datetime(df['launched_at'], format='%m%d%Y').astype(str)\n",
    "    df['state_changed_at'] = pd.to_datetime(df['state_changed_at'], format='%m%d%Y').astype(str)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dupes(df):\n",
    "    df = df[~df.duplicated('id')]\n",
    "    df = df.reset_index().drop(columns='index')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completed_campaigns(df):\n",
    "    df = df[df['state'].isin(['failed', 'successful'])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X and y\n",
    "# X = df.drop(columns=['state','pledged', 'usd_pledged', 'state_changed_at', 'spotlight',\n",
    "#                      'converted_pledged_amount', 'source_url', 'backers_count', 'state',\n",
    "#                      'is_backing',\t'is_starrable', 'is_starred'])\n",
    "# y = df['state']\n",
    "\n",
    "# # X_train, X_val,y_train, y_val = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cd c:\\Users\\Hakuj\\Documents\\DataSets\\Kickstarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = get_a_few(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df = get_a_year(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Datetime research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Time is in seconds from epoch time\n",
    "\n",
    "# https://www.epochconverter.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Date manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df = datetime_convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df['launched_at'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df['created_at'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# (df['launched_at'] - df['created_at']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# (df['deadline'] - df['launched_at']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Duplicate research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df['id'].nunique(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df[df.duplicated('id')].sort_values('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break up dictionaries\n",
    "- There are dictionaries in some of the columns, let's break them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_dics = ['category', 'creator', 'location', 'photo', 'profile', 'urls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Category\n",
    "- Old method using map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_category(df):\n",
    "    df['category'] = df['category'].map(eval) # converts row values to dict\n",
    "    df_of_column = df[col].apply(pd.Series)  #Breaks dict up into columns\n",
    "    df_of_column.columns = [f'{col}_'+col_name for col_name in df_of_column.columns] #Rename cols\n",
    "    df.join(df_of_column)\n",
    "    return df.drop(columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['category'] = df['category'].map(eval) # converts row values to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['category'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = df['category'].apply(pd.Series) #Breaks dict up into columns into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.columns #Col names for copypaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = test.rename(columns={'id': 'cat_id', 'name': 'cat_name',\n",
    "#                     'slug': 'cat_slug', 'position': 'cat_position', 'parent_id': 'cat_parent_id',\n",
    "#                     'color': 'cat_color'}) #Rename cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = test.drop(columns='urls') #Drop the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test #Ready to concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['category'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_dict(df, col):\n",
    "    \"\"\"Takes in a DataFrame and a list of column\n",
    "    names and unpacks the 'dictionaries' into new columns\"\"\"\n",
    "#     for col in cols: #Loop over columns\n",
    "    df[col] = df[col].apply(json.loads)\n",
    "    df_of_column = df[col].apply(pd.Series)\n",
    "    df_of_column.columns = [f'{col}_'+col_name for col_name in df_of_column.columns]\n",
    "    df.join(df_of_column)\n",
    "    return df.drop(columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (test['location'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"id\":2543897,\"project_id\":2543897,\"state\":\"inactive\",\"state_changed_at\":1464927602,\n",
    "#  \"name\":null,\"blurb\":null,\"background_color\":null,\"text_color\":null,\n",
    "#  \"link_background_color\":null,\"link_text_color\":null,\"link_text\":null,\n",
    "#  \"link_url\":null,\"show_feature_image\":false,\n",
    "#  \"background_image_opacity\":0.8,\n",
    "#  \"feature_image_attributes\":{\"image_urls\":\n",
    "#                              {\"default\":\"https://ksr-ugc.imgix.net/assets/012/621/061/7baad730e3071f23e83b8f419fc27768_original.JPG?ixlib=rb-1.1.0&crop=faces&w=1552&h=873&fit=crop&v=1464927868&auto=format&frame=1&q=92&s=269af376aaa7d2d88d577ad2477fa309\",\n",
    "#                               \"baseball_card\":\"https://ksr-ugc.imgix.net/assets/012/621/061/7baad730e3071f23e83b8f419fc27768_original.JPG?ixlib=rb-1.1.0&crop=faces&w=560&h=315&fit=crop&v=1464927868&auto=format&frame=1&q=92&s=0c83ea1c807978fa63977e6fdadd7445\"}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.io.json.json_normalize(df['creator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(jsond.decode(df['creator'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Natural Language\n",
    "- Let's use some natural language processing on the blurb and slug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Blurb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Tokenize \n",
    "- removing punctuation and putting in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['blurb'] = df['blurb'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['blurb'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['blurb'] = df['blurb'].apply(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['blurb'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\" Removes stop words. (i.e. 'i', me, you, he)\"\"\"\n",
    "    words = [word for word in text if word not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['blurb'] = df['blurb'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Lemmatize \n",
    "- Reduces words down to root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['blurb'] = df['blurb'].apply(lambda x: word_lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datetime_convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_dupes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
